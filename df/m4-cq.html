<!DOCTYPE html><html><head><meta charset="UTF-8"><title>M4: Content Question (Assembly Language Fundamentals )</title><link rel="stylesheet" href="style.css"></head><body><div class="topic"><h1>M4: Content Question (Assembly Language Fundamentals )</h1><div><p><span>Post questions about fundamentals of Assembly&nbsp;language.</span><span>&nbsp;Use the first line of your post as the subject of the post.</span></p></div></div><table><tbody><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>shiny shark</td></tr><tr><td><p>Hello all! Chapter 3 has sizable content and I am wondering how you will tackle that in such a short time. I am planning on driving by homework - as I work through the problem solving I will cover textbook material but definitely need some ideas on how to maximize the learning. Thanks in advance!</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>secure salamander</td></tr><tr><td><p>I'm going to try to do homework at the same time as I'm reading through, but with it being 35 pages, it's gonna be rough. Doing the quiz might be best to just get it out of the way too.&nbsp;</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>statutory squirrel</td></tr><tr><td><p>Hi there! What I do, is quickly read the reading and then&nbsp; watch a few extra YouTube videos (when I am not studying) and/or do some small sample problems.&nbsp;</p></td></tr></tbody></table><table class="comment" style="margin-left: 80px;"><tbody><tr><td>shiny shark</td></tr><tr><td><p>Thanks, guys for chiming in! Yes, it is a bit rough but I am making progress!</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>tough toad</td></tr><tr><td><p>I break it up into short lessons so I can focus more and take notes. For example, I read 3.1-3.3 first while taking notes and then read the remaining chapter the next day. It helped me pay more attention to the textbook.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>puzzled pony</td></tr><tr><td><p>I find that homework questions/programming questions usually cover different parts of the chapter.</p><p>So I usually:</p><p>Read through the question -&gt; then read through the part of the chapter that covers that material -&gt; then work on the question.&nbsp;</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>envious earwig</td></tr><tr><td><p>I usually read the questions, then skim through the chapter and make sure I focus on what can help with the question. If the content is still confusing, I'd watch some Youtube videos. I skim so that I can effectively manage my time.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>implicit hoverfly</td></tr><tr><td><p>To understand the concept, I refer to books and teachers' lectures, and look up videos on YouTube for parts I don't understand well.</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>surviving takin</td></tr><tr><td><p><span style="font-size: 18pt;">64-bit vs. 2<sup>n</sup>-bit</span></p><p>In terms of programming, 32-bit is slowly shifting to 64-bit and with the technology we currently have, so why wouldn't there be innovations for 128-bit, 256-bit, etc programming? Would that increase speed &amp; performance?</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>loose llama</td></tr><tr><td><p>Signed 64 bit numbers are HUGE and will probably be enough for quite some time.&nbsp; For example, Linux has been using a 32 bit Epoch time clock.&nbsp; This is the number of seconds since 1/1/1970.&nbsp; Unfortunately, this<a class="inline_disabled external" href="https://en.wikipedia.org/wiki/Year_2038_problem" target="_blank" rel="noreferrer noopener"><span>32 bit clock will overflow in 2038</span><span aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline" title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an external site.)</span></a>.&nbsp; Of course, everything will switch over to 64 bit Epoch time to prevent catastrophic breakdown (though nuclear warhead launchers will likely still have old 32 bit clocks but nothing to worry about).&nbsp; So how long until the 64 bit timestamp wraps around and forces us to use 128 bit timestamps? &nbsp; Approximately 292 billion years from now.&nbsp; This is just one example of how large 64 bit values really are.</p><p>Obviously strings and BLOBs can be trillions of bytes so being able to manipulate them quickly is always useful.&nbsp; But as far numeric integer values, 64 bits is more than enough for typical purposes.&nbsp; Scientific computing is another issue altogether and there are cases were extremely high precision floating point numbers are very useful.&nbsp; But for gaming, internet and accounting?&nbsp; 64 bit will be useful for a very long time.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>spatial snipe</td></tr><tr><td><p>Unless clock speed increases, speed of the processor cannot be increased as per my understanding and readings.&nbsp; Any processor above 64 bit processor is not yet commercially available. As explained in other replies to your post, currently there is plenty of room till we catchup to the limitations of 64 bit processor, but in my opinion we may soon see 128 bit processor as well.&nbsp; I may think like multiple cores on a processor, CPU may perform multiple partitions of bits and create parallelism, which I am not entirely sure if that is possible.&nbsp;</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>consistent centipede</td></tr><tr><td><p>In the slides, Chapter 3 Slide 53, it talks about how MASM supports 64-bit programming, with some restrictions. In which scenarios would we use 64-bit instead of 32-bit? Will we be using 32 or 64 bit in this class?</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>loose llama</td></tr><tr><td><p>This class is 32 bit only.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>Cole Gannon</td></tr><tr><td><p>Nowadays, you probably only wanna be using 64 bit because that's what architecture most desktop processors use. When you write these 32 bit applications to run on windows 10 64 bit, windows actually emulates a 32 bit environment with it's cursed Windows on Windows (WoW6432) thing. For this class I guess we're just gonna use 32 bit because it's what the book says to do.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>close buzzard</td></tr><tr><td><p>Hi Michael,</p><p>The name of this class has "x86" which is 32 bit only.</p><p>In the applications we use, making them 64 bit means the app can use more memory and not be restricted to 2-4 GB. There is still 32 bit for compatibility as 32 bit applications can run on a 64 bit processor / OS but not the other way around. 32 bit OSes are very rare nowadays though.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>agricultural ant</td></tr><tr><td><p>It really depends on what processor you are running your code, there are 32-bit CPUs and 64 bit CPUs. 64 bit is useful when running more intensive workloads. You can run 32-bit code on 64 bit, but you cannot run 64bit code on 32 bit. In this class, we will be using 32bit.&nbsp;</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>spatial snipe</td></tr><tr><td><p>This is possible because only 32 bit can be used and the rest of the 32 bits can be ignored. The same will not be possible vice versa.&nbsp; x86 is old as per the slides, lectures and other resources, 32bit gives foundation of machine level which is not much different from 64 bit.&nbsp;</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>genuine gopher</td></tr><tr><td><p>Reserved words</p><p>Why can't reserved words be used as identifies?</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>surviving takin</td></tr><tr><td><p>For example in C++, if you name an integer variable "string" the compiler wouldn't be able to properly differentiate the actual function of a string, used to define multiple characters, and it being a variable name, therefore string is a reserved word. The same applies to assembly programming.</p></td></tr></tbody></table><table class="comment" style="margin-left: 80px;"><tbody><tr><td>Cole Gannon</td></tr><tr><td><p><code>string</code>is not a reserved word in C or C++. You can do<code>int string = 1</code>just fine. A good example of a reserved word in C++ would be<code>if</code>. As for the reason why reserved words exist in any language, that's just how the language works. The language uses those reserved words for it's own purposes. They're not user changable.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>impressive hummingbird</td></tr><tr><td><p>Hi Haley, heres a list of most of the reserved words in MASM</p><p>https://faculty.kfupm.edu.sa/coe/mudawar/coe205/manuals/MASMProgrGuide/D-ReservedWords.pdf</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>grumpy haddock</td></tr><tr><td><p>The reason reserved words can't be used as identifiers is that the compiler needs to be able to determine whether the code follows the syntax and rules of the language, and what code to generate. But if identifiers are used, the compiler may not be able to generate as you like. Additionally using reserved words are probably not descriptive enough to what the identifier is.</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>passive owl</td></tr><tr><td><p><span style="font-size: 18pt;">The character 'h' behind the code.</span></p><p>Example: str1 BYTE "Enter name: ",0Dh,0Ah&nbsp;</p><p>How can I understand the "h" that always behind the code "0D" &amp; "0A"?</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>loose llama</td></tr><tr><td><p>The "h" suffix indicates that the value is hexadecimal.&nbsp; For example in C/C++ you would use 0x1234 for hex, but in MASM you would use 1234h.&nbsp; If your hex values starts with an alpha character, then you need to prefix the value with a zero.&nbsp; So for example in C/C++ you could write 0xABCD, but MASM requires 0ABCDh.&nbsp; MASM would assume that ABCDh must be a constant, label or variable name.</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>overseas orca</td></tr><tr><td><p><strong>MOV Mnemonic</strong></p><p>The book describes the MOV mnemonic, which moves a value from one register to another. For example, "MOV eax, ebx" would move the value in the ebx register to eax. Does this overwrite eax completely, or is the value that was previously in eax get stored somewhere in memory?</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>supposed swordtail</td></tr><tr><td><p>This will overwrite eax completely. Since we did not manually specify to move the value of eax to another location, the value will be lost.</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>financial fowl</td></tr><tr><td><p>So for any x86 register, there is a term called "scratch" that allows any function to overwrite another and can use it for anything, no questions asked. In this case, the value in the ebx register completely overwrites the eax register and there is no trace of the former value in eax.&nbsp;</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>electoral dragon</td></tr><tr><td><p>Integer Literals</p><p>I had a question on the "encoded real"&nbsp; (r) radix. Is this the only way to store floats/numbers with a decimal point in assembly? And why is it classified as an integer literal if it is used to describe floats?</p><p>Thanks!</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>entitled earwig</td></tr><tr><td><p>The radix refers to the base of the number, such as hexadecimal (base 16), binary (base 2), etc. For example, typing in h at the end of the number (as the radix), lets you type in hexadecimal numbers. The radix is not necessarily used to describe floats, but to specify the base of the integer literal. Floats are described using real number literals, not integer literals.&nbsp;</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>enormous earthworm</td></tr><tr><td><p>Comments<br><br>How did ; become the indicator for a single line comment in Assembly but in other languages it marks the end of the line of code you are working on?</p></td></tr></tbody></table><table class="comment" style="margin-left: 40px;"><tbody><tr><td>conscious cattle</td></tr><tr><td>I think when the assembly language was first used, the keyboard had a limited character set and that forced the developers to use ; for single line comment in Assembly language. But with time, more characters were added and it helped modern programming languages to use different syntax for comments.<div><br></div><div>The main reason ; is popular as a statement terminator is because most of today’s programming languages are based on ALGOL, and it used that convention.</div><div><br></div><div>This discussion was pretty helpful.</div><div><div><a href="https://softwareengineering.stackexchange.com/questions/139482/why-are-statements-in-many-programming-languages-terminated-by-semicolons" class="external" target="_blank" rel="noreferrer noopener"><span>https://softwareengineering.stackexchange.com/questions/139482/why-are-statements-in-many-programming-languages-terminated-by-semicolons</span><span aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline" title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an external site.)</span></a><br></div></div></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>relaxed python</td></tr><tr><td><p><strong>FWORD</strong></p><p>I realized that FWORD is for the usage of 48-bit integer, but I'm wondering how do we initialize it in the code?</p><p>&nbsp;</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>loose llama</td></tr><tr><td><p>What about signed and unsigned QWORD?</p><p>Chapter 3 mentions the following values with signed and unsigned variations:</p><ul><li>BYTE, SBYTE - 8 bit unsigned and signed integer values</li><li>WORD, SWORD - 16 bit unsigned and signed integer values</li><li>DWORD, SDWORD - 32 bit unsigned and signed integer values</li></ul><p>But only one type for 64 values:</p><ul><li>QWORD - 64 bit integer value</li></ul><p>Why is there no signed variant for 64 bit values?</p><p>&nbsp;</p></td></tr></tbody></table></td></tr><tr><td><table class="comment" style="margin-left: 0px;"><tbody><tr><td>advanced anaconda</td></tr><tr><td><p>In the book chapter 3 page 62, it tries to explain how NOP is used to align the address of the third instruction to a doubleword boundary.</p><p>I am not clear how the address alignment is calculated. Hope someone can explain it.</p><p><img src="https://deanza.instructure.com/users/131648/files/5552371/preview?verifier=w5PfNybLGDrMi0dLWs2ZKFPSg2NfeZFhwqmc3g3m" alt="Screenshot 2021-07-11 194259.jpg" data-api-endpoint="https://deanza.instructure.com/api/v1/users/131648/files/5552371" data-api-returntype="File" style="max-width: 687px;" width="687" height="246">&nbsp;</p></td></tr></tbody></table></td></tr></tbody></table></body></html>